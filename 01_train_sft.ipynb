{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dfefd343",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pkr/miniconda3/envs/rl/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-10 18:21:15,918] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pkr/miniconda3/envs/rl/shared/python_compiler_compat/ld: cannot find -lcufile: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n"
     ]
    }
   ],
   "source": [
    "# import necessary packages\n",
    "import sys, os\n",
    "import torch \n",
    "from transformers import (pipeline,\n",
    "                          AutoTokenizer,\n",
    "                          AutoModelForCausalLM,\n",
    "                          DataCollatorWithPadding,\n",
    "                          Llama4ForConditionalGeneration,\n",
    "                          get_scheduler)\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# config\n",
    "sys.path.append('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "afcffd16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load llama4 tokenizer\n",
    "model_id = \"meta-llama/Llama-4-Scout-17B-16E-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.pad_token = tokenizer.eos_token # set pad token to eos token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d51a083",
   "metadata": {},
   "source": [
    "## 1. Setup\n",
    "* Load data with the huggingface `load_dataset` function. We are using a local file, but a remote path to the huggingface hub can also be passed\n",
    "* partition the data into training and testing sets\n",
    "* print an example input/output pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8dc6e7e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'question': 'What is the ENSEMBL id of dynein axonemal assembly factor 11?', 'answer': 'ENSG00000129295'}\n"
     ]
    }
   ],
   "source": [
    "# load qa pairs as dataset object from local file\n",
    "dataset = load_dataset('json', data_files='data/qa_pairs.json')\n",
    "\n",
    "# split the data into train and test sets\n",
    "dataset = dataset['train'].train_test_split(test_size=0.2)\n",
    "\n",
    "# load the first example from the training dataset\n",
    "example = dataset['train'][0]\n",
    "print(example)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e78b368d",
   "metadata": {},
   "source": [
    "## 2. Preprocessing\n",
    "* We will create a function to format our question/answer pairs to simulate a user/assistant interaction. \n",
    "    - We follow the json \"role\", \"content\" format provided on huggingface docs\n",
    "    - After json formatting, we use the `apply_chat_template` method to add special characters indicating the start of user and assistant interactions\n",
    "* Next, we apply the Llama4 tokenizer to our text data\n",
    "    - Tokenizers convert words into integer indices, which is how LLMs understand language\n",
    "    - We specify a `max_length` for our sequences to avoid them becoming too large\n",
    "    - For sequences below the `max_length`, we apply padding to make all sentence sizes even."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f9427d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a formatting function to convert the dataset into a chat format\n",
    "def format_chat(row):\n",
    "    row_json_inp = [{\"role\": \"user\", \"content\": row[\"question\"]}]\n",
    "    row_json_out = [{\"role\": \"assistant\", \"content\": row[\"answer\"]}]\n",
    "    row[\"input\"] = tokenizer.apply_chat_template(row_json_inp, tokenize=False)\n",
    "    row[\"target\"] = tokenizer.apply_chat_template(row_json_out, tokenize=False)\n",
    "    return row\n",
    "\n",
    "def preprocess_data(examples):\n",
    "    inp = examples[\"input\"]\n",
    "    out = examples[\"target\"]\n",
    "    tokenized_data = tokenizer(text=inp, \n",
    "                               text_target=out,\n",
    "                               padding='max_length', \n",
    "                               truncation=True, \n",
    "                               max_length=512)\n",
    "    return tokenized_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f3800e24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'question': 'What is the ENSEMBL id of dynein axonemal assembly factor 11?', 'answer': 'ENSG00000129295', 'input': '<|begin_of_text|><|header_start|>user<|header_end|>\\n\\nWhat is the ENSEMBL id of dynein axonemal assembly factor 11?<|eot|>', 'target': '<|begin_of_text|><|header_start|>assistant<|header_end|>\\n\\nENSG00000129295<|eot|>'}\n",
      "{'input_ids': [200000, 200000, 200005, 1556, 200006, 368, 3668, 373, 290, 8940, 1546, 13346, 56, 1182, 323, 14764, 588, 259, 4810, 261, 347, 278, 19543, 5437, 220, 825, 43, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': [200000, 200000, 200005, 140680, 200006, 368, 17231, 51, 1052, 6861, 27940, 3461, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008, 200008]}\n"
     ]
    }
   ],
   "source": [
    "# run the functions and print examples\n",
    "formatted_data = format_chat(example)\n",
    "tokenized_data = preprocess_data(formatted_data)\n",
    "\n",
    "print(formatted_data)\n",
    "print(tokenized_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8f384f1",
   "metadata": {},
   "source": [
    "### 2.1. Map Function\n",
    "* We apply the formatting and tokenization to the entire dataset using the `.map` method, which applies dataset transformations in parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "69dd8384",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 162031/162031 [00:26<00:00, 6058.03 examples/s]\n",
      "Map: 100%|██████████| 40508/40508 [00:06<00:00, 6073.72 examples/s]\n",
      "Map: 100%|██████████| 162031/162031 [01:01<00:00, 2654.44 examples/s]\n",
      "Map: 100%|██████████| 40508/40508 [00:13<00:00, 3074.08 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# apply the map function to format the dataset\n",
    "formatted_dataset = dataset.map(format_chat, remove_columns=dataset['train'].column_names)\n",
    "tokenized_dataset = formatted_dataset.map(preprocess_data, batched=True, remove_columns=formatted_dataset['train'].column_names)\n",
    "tokenized_dataset = tokenized_dataset.with_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f0802319",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([512])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset['train'][0]['input_ids'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "582f011f",
   "metadata": {},
   "source": [
    "## 3. Creating DataLoaders\n",
    "* We utilize pytorch dataloaders to feed multiple sentence to the model at a time in a \"batch\" format\n",
    "    - We implement a collate function to ensure all sequences have the same length\n",
    "    - We make one dataloader for our training data, and one for our test data\n",
    "    - The shape of our inputs should now be `batch size` x `max_seq_len`, as seen in the example batch shape  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "07ccd57e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "batch_size = 2\n",
    "train_dataloader = DataLoader(tokenized_dataset['train'],\n",
    "                            batch_size=batch_size)\n",
    "\n",
    "val_dataloader = DataLoader(tokenized_dataset['test'],\n",
    "                            batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "45088e7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 512])\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(train_dataloader))\n",
    "print(batch['input_ids'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33c6e0b3",
   "metadata": {},
   "source": [
    "## 4. Example Forward Pass\n",
    "* We give an example to the model and examine the output it produces.\n",
    "    - Calling `model()` will perform a forward pass. \n",
    "        * The model takes token indices and predicts the probability for the next token in a sequence given all previous tokens.\n",
    "        * We can view the loss and logits. \n",
    "            - **The loss is a measure of how far the model's predicted next tokens deviate from the ground truth. A lower loss value indicates less error.**\n",
    "            - The logits are the raw predicted values for each token at each point in the sequence. We can convert our logits into probabilities by taking the softmax of the logits, and use this probability distribution to sample the next tokens in the sequence.\n",
    "    - Calling `model.generate()` will produce a text output.\n",
    "        * Under the hood, the model converts predicted logits to tokens and maps these tokens back into words with the tokenizer. This produces human-readable text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a8b38d65",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 50/50 [00:51<00:00,  1.04s/it]\n"
     ]
    }
   ],
   "source": [
    "model = Llama4ForConditionalGeneration.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "06abf354",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(25.3750, dtype=torch.bfloat16)\n",
      "torch.Size([2, 512, 202048])\n"
     ]
    }
   ],
   "source": [
    "# example forward pass (on a single example - there's a bug in llama4)\n",
    "with torch.no_grad():\n",
    "    outputs = model(input_ids=batch['input_ids'], \n",
    "                    attention_mask=batch['attention_mask'], \n",
    "                    labels=batch['labels'])\n",
    "    loss = outputs.loss\n",
    "    logits = outputs.logits\n",
    "    print(loss), print(logits.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "13e41f21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user\n",
      "\n",
      "What is the ENSEMBL id of dynein axonemal assembly factor 11?assistant\n",
      "\n",
      "I don't have the specific information on the ENSEMBL ID for dynein axonemal assembly factor 11. For the most accurate and up-to-date information, I recommend checking directly with the ENSEMBL database or other reliable genetic databases. They should have the most current details on gene identifiers, including ENSEMBL IDs for various genes and proteins.\n"
     ]
    }
   ],
   "source": [
    "# example generate (just for a single question here)\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(input_ids=batch['input_ids'].to('cuda'), \n",
    "                             attention_mask=batch['attention_mask'].to('cuda'), \n",
    "                             max_length=1024, \n",
    "                             do_sample=True, \n",
    "                             top_k=50, \n",
    "                             top_p=0.95, \n",
    "                             temperature=1.0)\n",
    "    print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
