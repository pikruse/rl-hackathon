{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dfefd343",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pkr/miniconda3/envs/rl/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-15 11:44:33,367] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pkr/miniconda3/envs/rl/shared/python_compiler_compat/ld: cannot find -lcufile: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n"
     ]
    }
   ],
   "source": [
    "# import necessary packages\n",
    "import sys, os\n",
    "import re\n",
    "import torch \n",
    "from transformers import (pipeline,\n",
    "                          AutoTokenizer,\n",
    "                          AutoModelForCausalLM,\n",
    "                          DataCollatorWithPadding,\n",
    "                          DataCollatorForLanguageModeling,\n",
    "                          Llama4ForConditionalGeneration,\n",
    "                          get_scheduler)\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# config\n",
    "sys.path.append('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "afcffd16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load llama4 tokenizer\n",
    "model_id = \"meta-llama/Llama-4-Scout-17B-16E-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.pad_token = tokenizer.eos_token # set pad token to eos token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d51a083",
   "metadata": {},
   "source": [
    "## 1. Setup\n",
    "* Load data with the huggingface `load_dataset` function. We are using a local file, but a remote path to the huggingface hub can also be passed\n",
    "* partition the data into training and testing sets\n",
    "* print an example input/output pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8dc6e7e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'question': 'What is the name of the gene with the symbol of MCM9?', 'answer': 'minichromosome maintenance 9 homologous recombination repair factor'}\n"
     ]
    }
   ],
   "source": [
    "# load qa pairs as dataset object from local file\n",
    "dataset = load_dataset('json', data_files='data/qa_pairs.json')\n",
    "\n",
    "# split the data into train and test sets\n",
    "dataset = dataset['train'].train_test_split(test_size=0.2)\n",
    "\n",
    "# load the first example from the training dataset\n",
    "example = dataset['train'][0]\n",
    "print(example)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e78b368d",
   "metadata": {},
   "source": [
    "## 2. Preprocessing\n",
    "* We will create a function to format our question/answer pairs to simulate a user/assistant interaction. \n",
    "    - We follow the json \"role\", \"content\" format provided on huggingface docs\n",
    "    - After json formatting, we use the `apply_chat_template` method to add special characters indicating the start of user and assistant interactions\n",
    "* Next, we apply the Llama4 tokenizer to our text data\n",
    "    - Tokenizers convert words into integer indices, which is how LLMs understand language\n",
    "    - We combine the `prompt` and `answer` to form a single string of text\n",
    "    - The `prompt` tokens are not included in our label calculations, since we expect a prompt as user input.\n",
    "    - We specify a `max_length` for our sequences to avoid them becoming too large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f9427d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a formatting function to convert the dataset into a chat format\n",
    "def format_chat(row):\n",
    "    row_json_inp = [{\"role\": \"user\", \"content\": row[\"question\"]}]\n",
    "    row_json_out = [{\"role\": \"assistant\", \"content\": row[\"answer\"]}]\n",
    "    row[\"prompt\"] = tokenizer.apply_chat_template(row_json_inp, tokenize=False)\n",
    "    row[\"response\"] = tokenizer.apply_chat_template(row_json_out, tokenize=False)\n",
    "    return row\n",
    "\n",
    "def preprocess_data(examples):\n",
    "    # concat to get full text\n",
    "    full_text = example[\"prompt\"] + example[\"response\"]\n",
    "\n",
    "    # tokenize\n",
    "    tokenized = tokenizer(full_text,\n",
    "                          truncation=True,\n",
    "                          max_length=512,\n",
    "                          add_special_tokens=False\n",
    "                          )\n",
    "    \n",
    "    # tokenize prompt and get length for loss masking\n",
    "    prompt_tokenized = tokenizer(example[\"prompt\"],\n",
    "                          truncation=True,\n",
    "                          max_length=512,\n",
    "                          add_special_tokens=False\n",
    "                          ) \n",
    "    prompt_length = len(prompt_tokenized['input_ids'])\n",
    "\n",
    "    # copy input_ids to get labels, set to -100 for loss masking\n",
    "    labels = tokenized['input_ids'].copy()\n",
    "    labels[:prompt_length] = [-100] * prompt_length\n",
    "    tokenized['labels'] = labels\n",
    "\n",
    "    return tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f3800e24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'question': 'What is the name of the gene with the symbol of MCM9?', 'answer': 'minichromosome maintenance 9 homologous recombination repair factor', 'prompt': '<|begin_of_text|><|header_start|>user<|header_end|>\\n\\nWhat is the name of the gene with the symbol of MCM9?<|eot|>', 'response': '<|begin_of_text|><|header_start|>assistant<|header_end|>\\n\\nminichromosome maintenance 9 homologous recombination repair factor<|eot|>'}\n",
      "{'input_ids': [200000, 200005, 1556, 200006, 368, 3668, 373, 290, 1327, 323, 290, 14561, 517, 290, 9985, 323, 376, 6877, 37, 43, 200008, 200000, 200005, 140680, 200006, 368, 1419, 665, 154899, 19277, 220, 37, 192161, 108592, 21769, 5437, 200008], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 200000, 200005, 140680, 200006, 368, 1419, 665, 154899, 19277, 220, 37, 192161, 108592, 21769, 5437, 200008]}\n"
     ]
    }
   ],
   "source": [
    "# run the functions and print examples\n",
    "formatted_data = format_chat(example)\n",
    "tokenized_data = preprocess_data(formatted_data)\n",
    "\n",
    "print(formatted_data)\n",
    "print(tokenized_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8f384f1",
   "metadata": {},
   "source": [
    "### 2.1. Map Function\n",
    "* We apply the formatting and tokenization to the entire dataset using the `.map` method, which applies dataset transformations in parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "69dd8384",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 162031/162031 [00:26<00:00, 6100.13 examples/s]\n",
      "Map: 100%|██████████| 40508/40508 [00:06<00:00, 6190.17 examples/s]\n",
      "Map: 100%|██████████| 162031/162031 [00:36<00:00, 4458.00 examples/s]\n",
      "Map: 100%|██████████| 40508/40508 [00:08<00:00, 4518.14 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# apply the map function to format the dataset\n",
    "formatted_dataset = dataset.map(format_chat, remove_columns=dataset['train'].column_names)\n",
    "tokenized_dataset = formatted_dataset.map(preprocess_data, remove_columns=[\"prompt\", \"response\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0b5874b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prompt': '<|begin_of_text|><|header_start|>user<|header_end|>\\n\\nWhat is the name of the gene with the symbol of MCM9?<|eot|>',\n",
       " 'response': '<|begin_of_text|><|header_start|>assistant<|header_end|>\\n\\nminichromosome maintenance 9 homologous recombination repair factor<|eot|>'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "formatted_dataset['train'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "582f011f",
   "metadata": {},
   "source": [
    "## 3. Creating DataLoaders\n",
    "* We utilize pytorch dataloaders to feed multiple sentence to the model at a time in a \"batch\" format\n",
    "    - We implement a collate function to ensure all sequences have the same length\n",
    "    - We make one dataloader for our training data, and one for our test data\n",
    "    - The shape of our inputs should now be `batch size` x `batch_max_seq_len`, as seen in the example batch shape  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "07ccd57e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "batch_size = 2\n",
    "train_dataloader = DataLoader(tokenized_dataset['train'],\n",
    "                              collate_fn=data_collator,\n",
    "                              batch_size=batch_size)\n",
    "\n",
    "val_dataloader = DataLoader(tokenized_dataset['test'],\n",
    "                            collate_fn=data_collator,\n",
    "                            batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "45088e7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 37])\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(train_dataloader))\n",
    "print(batch['input_ids'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33c6e0b3",
   "metadata": {},
   "source": [
    "## 4. Example Forward Pass\n",
    "* We give an example to the model and examine the output it produces.\n",
    "    - Calling `model()` will perform a forward pass. \n",
    "        * The model takes token indices and predicts the probability for the next token in a sequence given all previous tokens.\n",
    "        * We can view the loss and logits. \n",
    "            - **The loss is a measure of how far the model's predicted next tokens deviate from the ground truth. A lower loss value indicates less error.**\n",
    "            - The logits are the raw predicted values for each token at each point in the sequence. We can convert our logits into probabilities by taking the softmax of the logits, and use this probability distribution to sample the next tokens in the sequence.\n",
    "    - Calling `model.generate()` will produce a text output. \n",
    "        * Under the hood, the model converts predicted logits to tokens and maps these tokens back into words with the tokenizer. This produces human-readable text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a8b38d65",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 50/50 [00:51<00:00,  1.03s/it]\n"
     ]
    }
   ],
   "source": [
    "model = Llama4ForConditionalGeneration.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "25c3d062",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 37]) torch.Size([2, 37]) torch.Size([2, 37])\n"
     ]
    }
   ],
   "source": [
    "print(batch['input_ids'].shape, batch['labels'].shape, batch['attention_mask'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "06abf354",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(7.2188, dtype=torch.bfloat16)\n",
      "torch.Size([2, 37, 202048])\n"
     ]
    }
   ],
   "source": [
    "# example forward pass (on a single example - there's a bug in llama4)\n",
    "with torch.no_grad():\n",
    "    outputs = model(**batch)\n",
    "    loss = outputs.loss\n",
    "    logits = outputs.logits\n",
    "    print(loss), print(logits.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b6516de9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is the symbol of A-kinase anchoring protein 8 pseudogene 1?\n",
      "Generated Answer: The symbol for A-kinase anchoring protein 8 pseudogene 1 is AKAP8P1.\n",
      "Expected Answer: AKAP8P1\n"
     ]
    }
   ],
   "source": [
    "# make generation function\n",
    "def generate_response(prompt):\n",
    "    # tokenize and generate\n",
    "    prompt_text = tokenizer.apply_chat_template([{\"role\": \"user\", \"content\": prompt}], tokenize=False)\n",
    "    tokenized = tokenizer(prompt_text, return_tensors=\"pt\")\n",
    "    input_ids = tokenized['input_ids'].to(model.device)\n",
    "    attention_mask = tokenized['attention_mask'].to(model.device)\n",
    "    generated_ids = model.generate(input_ids,\n",
    "                                   attention_mask=attention_mask,\n",
    "                                   max_new_tokens=1024,\n",
    "                                   pad_token_id=tokenizer.eos_token_id,\n",
    "                                   temperature=0.1)\n",
    "    full_output = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "\n",
    "    # get answer tokens and remove \"assistant\" prefix\n",
    "    prompt_length = len(tokenizer(prompt_text, add_special_tokens=False)['input_ids'])\n",
    "    answer_ids = generated_ids[0][prompt_length:]\n",
    "    answer_text = tokenizer.decode(answer_ids, skip_special_tokens=True).strip()\n",
    "    cleaned_answer = re.sub(r\"^assistant[\\s:]+\", \"\", answer_text, flags=re.IGNORECASE)\n",
    "\n",
    "    return cleaned_answer\n",
    "\n",
    "question = \"What is the symbol of A-kinase anchoring protein 8 pseudogene 1?\"\n",
    "answer = generate_response(question)\n",
    "print(f\"Question: {question}\")\n",
    "print(f\"Generated Answer: {answer}\")\n",
    "print(f\"Expected Answer: AKAP8P1\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
